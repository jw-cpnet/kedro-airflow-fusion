from __future__ import annotations
import os
from datetime import datetime, timedelta
from pathlib import Path

from airflow import DAG
from airflow.models import BaseOperator

from kedro.framework.session import KedroSession
from kedro.framework.project import configure_project


os.environ["KEDRO_ENV"] = "{{ env }}"

class KedroOperator(BaseOperator):
    def __init__(
        self,
        package_name: str,
        pipeline_name: str,
        node_name: str | list[str],
        project_path: str | Path,
        env: str,
        *args, **kwargs
    ) -> None:
        super().__init__(*args, **kwargs)
        self.package_name = package_name
        self.pipeline_name = pipeline_name
        self.node_name = node_name
        self.project_path = project_path
        self.env = env

    def execute(self, context):
        configure_project(self.package_name)
        with KedroSession.create(project_path=f"{self.package_name}/{self.pipeline_name}",
                                 conf_source=f"/opt/airflow/{self.package_name}/{self.pipeline_name}/conf",
                                 env=self.env) as session:
            if isinstance(self.node_name, str):
                self.node_name = [self.node_name]
            session.run(self.pipeline_name, node_names=self.node_name)


# Kedro settings required to run your pipeline
env = "{{ env }}"
pipeline_name = "{{ pipeline_name }}"
project_path = "/app/fusion"
package_name = "fusion"

# Using a DAG context manager, you don't have to specify the dag property of each task
with DAG(
    dag_id="{{ dag_name | safe | slugify }}-{{ pipeline_name | safe | slugify }}-{{ env | safe | slugify }}",
    start_date=datetime({{ start_date | default([2024, 1, 1]) | join(',') }}),
    max_active_runs={{ max_active_runs | default(1) }},
    # https://airflow.apache.org/docs/stable/scheduler.html#dag-runs
    # https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#scheduling
    schedule="{{ schedule_interval | default('@once') }}",
    catchup={{ catchup | default(False) }},
    # Default settings applied to all tasks
    default_args=dict(
        owner="{{ owner | default('airflow') }}",
        depends_on_past={{ depends_on_past | default(False) }},
        email_on_failure={{ email_on_failure | default(False) }},
        email_on_retry={{ email_on_retry | default(False) }},
        retries={{ retries | default(1) }},
        retry_delay=timedelta(minutes={{ retry_delay | default(5) }})
    ),
    tags=[{{ pipeline_name }}],
) as dag:
    tasks = {
    {% for node_name, node_list in nodes.items() %}        "{{ node_name | safe | slugify }}": KedroOperator(
            task_id="{{ node_name | safe | slugify  }}",
            package_name=package_name,
            pipeline_name=pipeline_name,
            node_name={% if node_list | length > 1 %}[{% endif %}{% for node in node_list %}"{{ node.name | safe }}"{% if not loop.last %}, {% endif %}{% endfor %}{% if node_list | length > 1 %}]{% endif %},
            project_path=project_path,
            env=env,
        ),
{% endfor %}    }

    {% for parent_node, child_nodes in dependencies.items() -%}
    {% for child in child_nodes %}    tasks["{{ parent_node | safe | slugify }}"] >> tasks["{{ child | safe | slugify }}"]
    {% endfor %}
    {%- endfor %}
